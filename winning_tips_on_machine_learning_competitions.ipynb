{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Winning Tips on Machine Learning Competitions\n",
    "These tips are shared by Marlos Michailidis (a.k.a Kazanova), Kaggle Grandmaster, in a webinar on 5th March 2016. You can access the video and slides from this [tutorial](https://www.hackerearth.com/practice/machine-learning/advanced-techniques/winning-tips-machine-learning-competitions-kazanova-current-kaggle-3/tutorial/). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. What are the steps you follow for solving a ML problem? Please describe from scratch.**\n",
    "\n",
    "\n",
    "Following are the steps I undertake while solving any ML problem:\n",
    "\n",
    "1. Understand the data - After you download the data, start exploring features. Look at data types. Check variable classes. Create some univariate - bivariate plots to understand the nature of variables.\n",
    "2. Understand the metric to optimise - Every problem comes with a unique evaluation metric. It's imperative for you to understand it, specially how does it change with target variable.\n",
    "3. Decide cross validation strategy - To avoid overfitting, make sure you've set up a cross validation strategy in early stages. A nice CV strategy will help you get reliable score on leaderboard.\n",
    "4. Start hyper parameter tuning - Once CV is at place, try improving model's accuracy using hyper parameter tuning. It further includes the following steps:\n",
    "  * Data transformations: It involve steps like scaling, removing outliers, treating null values, transform categorical variables, do feature selections, create interactions etc.\n",
    "  * Choosing algorithms and tuning their hyper parameters: Try multiple algorithms to understand how model performance changes.\n",
    "  * Saving results: From all the models trained above, make sure you save their predictions. They will be useful for ensembling.\n",
    "  * Combining models: At last, ensemble the models, possibly on multiple levels. Make sure the models are correlated for best results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. What are the model selection and data manipulation techniques you follow to solve a problem?**\n",
    "\n",
    "\n",
    "Generally, I try (almost) everything for most problems. In principle for:\n",
    "\n",
    "1. **Time series:** I use GARCH, ARCH, regression, ARIMA models etc.\n",
    "2. **Image classification:** I use deep learning (convolutional nets) in python.\n",
    "3. **Sound Classification:** Common neural networks\n",
    "4. **High cardinality categorical (like text data):** I use linear models, FTRL, Vowpal wabbit, LibFFM, libFM, SVD etc.\n",
    "5. For everything else,I use Gradient boosting machines (like XGBoost and LightGBM) and deep learning (like keras, Lasagne, caffe, Cxxnet). I decide what model to keep/drop in Meta modelling with feature selection techniques. Some of the feature selection techniques I use includes:\n",
    "  * Forward (cv or not) - Start from null model. Add one feature at a time and check CV accuracy. If it improves keep the variable, else discard.\n",
    "  * Backward (cv or not) - Start from full model and remove variables one by one. It CV accuracy improves by removing any variable, discard it.\n",
    "  * Mixed (or stepwise) - Use a mix of above to techniques.\n",
    "  * Permutations\n",
    "  * Using feature importance - Use random forest, gbm, xgboost feature selection feature.\n",
    "  * Apply some stats’ logic such as chi-square test, anova.\n",
    "\n",
    "Data manipulation technique could be different for every problem :\n",
    "\n",
    "* **Time series:** You can calculate moving averages, derivatives. Remove outliers.\n",
    "* **Text:** Useful techniques are tfidf, countvectorizers, word2vec, svd (dimensionality reduction). Stemming, spell checking, sparse matrices, likelihood encoding, one hot encoding (or dummies), hashing.\n",
    "* **Image classification:** Here you can do scaling, resizing, removing noise (smoothening), annotating etc\n",
    "* **Sounds:** Calculate Furrier Transforms , MFCC (Mel frequency cepstral coefficients), Low pass filters etc\n",
    "* **Everything else:** Univariate feature transformations (like log +1 for numerical data), feature selections, treating null values, removing outliers, converting categorical variables to numeric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Can you elaborate cross validation strategy?**\n",
    "\n",
    "Cross validation means that from my main set, I create RANDOMLY 2 sets. I built (train) my algorithm with the first one (let’s call it training set) and score the other (let’s call it validation set). I repeat this process multiple times and always check how my model performs on the test set in respect to the metric I want to optimise.\n",
    "\n",
    "The process may look like:\n",
    "\n",
    "* For 10 (you choose how many X) times\n",
    "* Split the set in training (50%-90% of the original data)\n",
    "* And validation (50%-10% of the original data)\n",
    "* Then fit the algorithm on the training set\n",
    "* Score the validation set. \n",
    "* Save the result of that scoring in respect to the chosen metric.\n",
    "* Calculate the average of these 10 (X) times. That how much you expect this score in real life and is generally a good estimate.\n",
    "* Remember to use a SEED to be able to replicate these X splits\n",
    "* Other things to consider is Kfold and stratified KFold . Read here. For time sensitive data, make certain you always the rule of having past predicting future when testing’s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
